---
title: "Build Predictive Models to Predict Insurance Charges"
author: "Rakesh Karki"
output: pdf_document
---

**INTRODUCTION:**  
In this project, we will analyze data to build various predictive models to predict the insurance change of an individual based on 'Age', 'Sex', 'BMI', 
'Number of Children', 'Smoker status', ' Region of residence', 'Current Insurance Charges' as features/predictors.   
We will randomly split the data into training (2/3rd of the data) and testing (1/3rd of the data). We will then create multiple "linear regression model",
"regression tree model", "random forest model", "support vector machine (SVM)", "K-means cluster analysis" and "neural network model" using the
training data. We will then calculate the Mean Square Error (MSE) value of each of the models using out of sample (testing) data and select the model with
the lowest MSE as best model to predict the insurance charge.  
This is a relatively small size data for easy demonstration, but the process can be applied to a large size as well. 

**Analysis Flow:**    
**STEP 1. DATA PREPARATION**  
**STEP 2. BUILD A MULTIPLE LINEAR REGRESSION MODEL**  
**STEP 3. BUILD A REGRESSION TREE MODEL**  
**STEP 4. BUILD A RANDOM FOREST MODEL**  
**STEP 5. BUILD A SUPPORT VECTOR MACHINE (SVM)**  
**STEP 6. PERFORM THE K-MEANS CLUSTER ANALYSIS**  
**STEP 7. BUILD THE NEURAL NETWORK MODEL**  
**STEP 8. PUTTING IT ALL TOGETHER**  

```{r, echo = FALSE }
rm(list=ls())    # clear the R memory
```

```{r, setup, Include = FALSE}
knitr::opts_chunk$set(include=TRUE, error=FALSE, message=FALSE,echo = TRUE)
```
################################################################################  
# **STEP 1.  DATA PREPARATION**

## Step 1-a. Load the table **insurance.csv** into R and then preview the data to understand the structure.
```{r}
library(readr)
insurance <- read_csv("insurance.csv")
dim(insurance)       # check the data size: rowsXcolumns
head(insurance)      # preview first few rows
```

## Step 1-b.  Log transform the label variable 'charges'. This helps the analysis by converting the skewed data towards normal distribution. 
```{r}
insurance$charges = log(insurance$charges)
```

### Check the data after the log transformation.
```{r}
head(insurance)
```
## Step 1-c. Use model.matrix() function to transform the data from 1-b above to create another data set that uses dummy variables in place of categorical variables. This transformation is needed to build the K-means cluster and neural network model later. Then, verify that the first column has only ones (1) as values, and then discard the first column as it does not have any meaning on future analysis.

### Convert the data to matrix using **model.matrix() function**.
```{r}
insurance.matrix<-model.matrix(~., data=insurance)
```

### Verify the conversion.
```{r}
head(insurance.matrix)
```
### Remove the first column from the matrix and check if the first column has been removed from the DataFrame.
```{r}
insurance.matrix<- insurance.matrix[, -1]
head(insurance.matrix)
```
## Step 1-d. Use the **sample() function** to generate row indexes for the training and tests sets: 2/3 of the row indexes for the training set and 1/3 for the test set.

### Set the seed for result reproducibility.
```{r}
set.seed(1)
```

### Randomly sample the 2/3rd of the data rows as training set.
```{r}
training.rows<- sample(1:nrow(insurance), nrow(insurance)*2/3)
```

## Step 1-e.  Create a training and test data from the data created in 1.b using the training and test row indexes created at 1-d above.

### Training data.
```{r}
training.data<- insurance[training.rows, ]
```

### Preview the training data.
```{r}
dim(training.data)
head(training.data)
```

### Testing data.
```{r}
test.data<- insurance[-training.rows, ]
```

### Preview the testing data.
```{r}
dim(test.data)
head(test.data)
```

## Step 1-f.  Create training and test DataFrames from matrix DataFrame created at 1-c using the training and test row indexes created at 1-d

### Training data - matrix.
```{r}
training.matrix<- insurance.matrix[training.rows, ]
```

### Preview the training data
```{r}
dim(training.matrix)
head(training.matrix)
```
### Testing data - matrix.
```{r}
testing.matrix<- insurance.matrix[-training.rows, ]
```

### Preview the testing data.
```{r}
dim(testing.matrix)
head(testing.matrix)
```
################################################################################
# **STEP 2.  BUILD A MULTIPLE LINEAR REGRESSION MODEL**

## Step 2-a.  Here, I will build a multiple linear regression model with 'charges' as the response/label and 'age', 'sex', 'bmi', 'children', 'smoker', and 'region' as predictor/feature variables. I will then print out the result using the summary() function. 

### First convert the categorical variables ('sex', 'smoker' and 'region') to factor.
```{r}
insurance$sex<-factor(insurance$sex)
insurance$smoker<-factor(insurance$smoker)
insurance$region<-factor(insurance$region)
```

### Then, build a multiple linear regression model with predictors and response as defined above using the training data and print out the result using the summary() function.
```{r}
lm <- lm(charges ~ age + sex + bmi+ children +smoker + region, data = training.data)
summary(lm)
```
## Step 2-b. Check the relationship between the predictor variables and the response (find the coefficients).

### Coefficients of variables of the multiple linear regression model.
```{r}
lm$coefficients
```

### **As the coefficient of each variables are non zero, each predictors has effect on the response variable. The p values above (with 1 or more * sign) also show that effect of each of predictor variable is statistically significant.**

## Step 2-c.  Now, perform best subset selection using the stepAIC() function from the "MASS" library and choose best model based on AIC. For the "direction" parameter in the stepAIC() method, I will set direction = "backward"

### Load package "MASS" into R memory.
```{r}
library(MASS)
```

### Build the multiple linear regression model using all predictor variables.
```{r}
full.lm = lm(charges ~ age + sex + bmi+ children +smoker + region, data = training.data)
```

### Run backward feature selection calling the function stepAIC. 
```{r}
lm.bwd = stepAIC(full.lm, direction = "backward")
```
### Print the result.
```{r}
summary(lm.bwd)
```
### **As all of the variables are statistically significant based on P value above (confirmed again), the model should include all feature variables: 'age', 'sex', 'bmi', 'children', 'smoker' and 'region' as the predictors.**

## Step 2-d.  Here, I will compute the MSE of the best model built in Step 2-c above based on AIC using Leave-One-Out Cross-Validation (LOOCV) by trainControl() and train() function of the library 'caret'. I will then calcuate the Mean Square Error (MSE) by squaring the reported RMSE.

### Load package 'caret' into R memory.
```{r}
library(caret)
```

### Create the trainControl with Leave-One-Out Cross-Validation (LOOCV) method.
```{r}
train.control.lm1 <- trainControl(method="LOOCV")
```

### Identify the best linear regression model using the Leave-One-Out Cross-Validation (LOOCV) method.
```{r}
model.lm1 <- train(charges ~ age + sex + bmi+ children +smoker + region, data = training.data,
               trControl=train.control.lm1, method="lm")
print(model.lm1)
```
### Here, calculate MSE as square of RMSE calculated above.
```{r}
MSE.lm1<- 0.4282153^2
print(MSE.lm1)
```

## Step 2-e.Now, calculate the MSE of the best model based on AIC using 10-fold Cross-Validation. Then calcualte the MSE.

### Create the trainControl method with 10-fold Cross-Validation.
```{r}
train.control.lm2 <- trainControl(method="CV", number = 10)
```

### Indentify the best linear regression model with 10-fold Cross-Validation.
```{r}
model.lm2 <- train(charges ~ age + sex + bmi+ children +smoker + region, data = training.data,
                 trControl=train.control.lm2, method="lm")
```

### Print the result.
```{r}
print(model.lm2)
```
### Let's calcucalte the MSE.
```{r}
MSE.lm2<- 0.4260563^2
print(MSE.lm2)
```

## tep 2-f.  Calculate and report the test MSE using the best model from Step 2-c and the test data set from step 1.e.

### Predict the insurance charges using the test data. 
```{r}
yhat.insurance.charges.lm <- predict(lm.bwd, newdata = test.data)
```

### Identify the actual insurance charges from the test data.
```{r}
test.insurance.charges <- insurance[-training.rows, "charges"]
```

### Calculate the mean square error (MSE) of the model using out of sample (test) data.
```{r}
MSE.Test.lm<-mean((test.insurance.charges$charges-yhat.insurance.charges.lm)^2)
print(MSE.Test.lm)
```

### **The calculated MSE with test data is 0.231291 which is larger than the calculated MSE using training data (as expected - as model tends to perform better with training data than Out of Sample - test data).**
################################################################################
# **STEP 3. BUILD A REGRESSION TREE MODEL**

## Step 3-a. Now, Build a regression tree model using function tree(), with 'charges' as the response variables and the 'age', 'sex', 'bmi', 'children', 'smoker', and 'region' as predictors.

### Load the package "tree" into R memory.
```{r}
library(tree)
```

### Now, build the regression tree model using the training data and view the model.
```{r}
tree.train<-tree(charges ~ age + sex + bmi+ children + smoker + region, insurance, 
        subset = training.rows)
print(tree.train)
```

## Step 3-b.  Now, I will find the optimal tree using cross-validation and display the result graphically. will then determine the best tree size.

### Perform the cross-validation to choose the optimal tree.
```{r}
train.insurance.cv <- cv.tree(tree.train)
```

### Plot the model results from Step 3-b and determine the best optimal tree size. 
```{r,warning=FALSE}
plot(train.insurance.cv$size, train.insurance.cv$dev, ttpe = "b")
```

### Looking at the graph, it seems that the optimal tree size is 4. Best tree is the tree with the lowest test error (y value of the dev vs size graph). Even though the tree of size 8 has the lowest test error, the test error is not much better than the tree of size 4. Model with tree size 8 will have low bias as it captures variation on most of the training data while doing so it will over fit the model and the model will also be more complex than tree of size 4. Because of these reasons, I will select the tree size of 4 as the optimal model.

## Step 3-c. Now, I will prune the tree using the optinal size (4) determined above.
```{r}
prune.train.insurance<- prune.tree(tree.train, best = 4)
```

## Step 3-d. Now, visually display the best tree model with labels.

### As the variable 'charges' was log transformed at the beginning of the analysis, it needs to be converted it back by using the exp() to make the predicted output meaningful.

### i. Copy the pruned tree model to a new variable.
```{r}
copy.prune.train<-prune.train.insurance
```

### ii.  Reverse the log transformation of column yval using the exp() function.

```{r}
copy.prune.train$frame$yval<-exp(copy.prune.train$frame$yval)
```

###  iii. Now, display the optimal  regression tree model.

```{r}
library(tree)
plot(copy.prune.train)
text(copy.prune.train, pretty = 0)
```

## Step 3-e. Calculate the test MSE for the best model.

### Let's predict the charges using the test data.
```{r}
yhat.insurance.charges.tree <- predict(prune.train.insurance, newdata = insurance[-training.rows, ])
```

### Let's find the actual charges value from the test data.
```{r}
test.insurance.charges <- insurance[-training.rows, "charges"]
```

### Now, let's calculate the mean square error (MSE) with the test data.
```{r}
MSE.Test.tree <-mean((test.insurance.charges$charges-yhat.insurance.charges.tree)^2)
print(MSE.Test.tree)
```

################################################################################
# **STEP 4. BUILD A RANDOM FOREST MODEL**

## Step 4-a.  Here, I will build a random forest model using function randomForest(), with 'charges' as the response variables and the 'age', 'sex', 'bmi', 'children', 'smoker', and 'region' as predictors.

### Load the package "randomForest" into R memory.
```{r}
library(randomForest)
```

### Build the random forest model. 
```{r}
rf.model <- randomForest(charges ~ age + sex + bmi+ children + smoker + region, 
            data = insurance, subset = training.rows, importance = TRUE)
```

## Step 4-b. Calculate the MSE using the test data.

### Predict insurance charge based using the test data.
```{r}
yhat.insurance.charges.rf <- predict(rf.model, newdata = insurance[-training.rows, ])
```

### Find the actual insurance charge from the test data.
```{r}
test.insurance.charges <- insurance[-training.rows, "charges"]
```

### Calculate the mean square error (MSE) with test data.
```{r}
MSE.Test.rf<- mean((test.insurance.charges$charges-yhat.insurance.charges.rf)^2)
print(MSE.Test.rf)
```

## Step 4-c.  Extract variable importance measure using the importance() function.
```{r}
importance(rf.model)
```
## Step 4-d. Plot the variable importance and identify the top 3 important predictors in this model.
```{r}
varImpPlot(rf.model)
```

### Top three most important predictors of insurance charges are smoker, age and children (Based on %IncMSE) or bmi(Based on IncNodePurity).

################################################################################

# **STEP 5. BUILD A SUPPORT VECTOR MACHINE (SVM) MODEL**

## Step 5-a.  Build the SVM with 'charges' as the response variables and the 'age', 'sex', 'bmi', 'children', 'smoker', and 'region' as predictors. We will use the svm() function with radial kernel and gamma = 5 and cost = 50.

### Load the package "e1071" into R memory.
```{r}
library(e1071)
```

### Build the SVM model and print the model summary.
```{r}
SVM.fit<- svm(charges ~ age + sex + bmi+ children + smoker + region, data = training.data, 
              kernel= "radial", cost=50, gamma=5)
summary(SVM.fit)
```

## Step 5-b.  Perform a grid search to find the best model with potential cost: 1, 10, 50, 100 and potential gamma: 1,3 and 5 and potential kernel: "linear","polynomial","radial" and "sigmoid".

```{r, message = FALSE, warning=FALSE}
tune.out<-tune(svm, charges ~ age + sex + bmi+ children + smoker + region, data = training.data, 
    kernel= c("linear","polynomial","radial", "sigmoid"), ranges = 
        list(cost=c(1,10,50,100), gamma=c(1,3,5)))
```

## Step 5-c.  Print out the model results and identify the best model parameters.
```{r}
summary(tune.out)
```

### **As shown on the output, the best model is the model with cost value 1 and gamma value 1.**

## Step 5-d.  Now, calcuate the MSE using the test data.

### Predict the charges using the test data.
```{r}
Predicted.charges.svm<- predict(tune.out$best.model, newdata = test.data)
```

### Find the actual charges from the test data.
```{r}
test.insurance.charges<- insurance[-training.rows, "charges"]
```

### Calcualte MSE with test data.
```{r}
MSE.Test.svm<-mean((test.insurance.charges$charges-Predicted.charges.svm)^2)
print(MSE.Test.svm)
```

 
################################################################################

# **STEP 6. PERFORM THE K-MEANS CLUSTER ANALYSIS**

## Step 6-a. Use the training data created in step 1.f (matrix tranformation) and standardize the inputs using the scale() function.

### Standarizing the data using scale() function.
```{r}
scale.training.matrix<-scale(training.matrix)
```

### Preview the data.
```{r}
head(scale.training.matrix)
```

## Step 6-b. Convert the standardized inputs to a FataFrame using the as.data.frame() function.
```{r}
scale.training.matrix<-as.data.frame(scale.training.matrix)
```

## Step 6-c. Determine the optimal number of clusters.

### Begin by loading the package "cluster" and "Factoextra" into R memory.
```{r}
library(cluster)
library(factoextra)
```

### Determine the optimal number of clusters. Use the gap_stat method and set iter.max=20.
```{r}
fviz_nbclust(scale.training.matrix, kmeans, method = "gap_stat", iter.max=20)
```

### As shown on the graph above, optimal level of cluster is 5. This is based on the change in the gap statistic vs the number of clusters. Increasing the number of clusters beyond 5 did not increase the Gap statistic significantly.

## Step 6-d.  Perform k-means clustering using the optimal number of clusters found in step 6.c. Set parameter nstart = 25

### Build K-means clustering.
```{r}
km.res<-kmeans(scale.training.matrix, 5, nstart = 25)
```

## Step 6-e.  Visualize the clusters in different colors, setting parameter geom = "point"

### Let's plot the clusters.
```{r}
fviz_cluster(km.res, data = scale.training.matrix, geom = "point")
```

################################################################################

# **STEP 7. BUILD THE NEURAL NETWORK MODEL**

## Step 7-a.  Using the training data set created in step 1.f (matrix tranformed), we will create a neural network model where the response is 'charges' and the predictors are 'age', 'sex', 'bmi', 'children', 'smoker', 'region'. We will use 1 hidden layer with 1 neuron.

### Load the package "neuralnet" in to R memory.
```{r}
library(neuralnet)
```

### Build the neural network model.
```{r}
nn.model<- neuralnet(charges~ age+sexmale+bmi+children+smokeryes+regionnorthwest+
    regionsoutheast+regionsouthwest, data = training.matrix, hidden=1)
```

## Step 7-b.  Plot the neural network.
```{r}
plot(nn.model)
```

## Step 7-c.  Calculate the MSE using the test data.

### Predict the charges in the test dataset.
```{r}
Predicted.charges.nm<-compute(nn.model, testing.matrix[ ,c("age","sexmale", "bmi", "children", 
    "smokeryes", "regionnorthwest", "regionsoutheast", "regionsouthwest")])
```

### Preview the predicted charges (log tranformed)
```{r}
head(Predicted.charges.nm$net.result)
```
### Actual charges value (log tranformed) from test data.
```{r}
observ.test<- testing.matrix[ ,"charges"]
head(observ.test)
```

### Compute the Mean Square Error (MSE) using the test data.
```{r}
MSE.Test.nm<-mean((observ.test-Predicted.charges.nm$net.result)^2)
print(MSE.Test.nm)
```

################################################################################

# **STEP 8. PUTTING IT ALL TOGETHER**

## Step 8-a. Let's compare the best models among multiple regression, regression tree, random forest, support vector machine, and neural network models using MSE of the models calcualted using test data as the performance indicator (lower is better). 

### Summarize the model performance in a DataFrame.

### Model type
```{r}
Model.Type<-c("Multiple Linear Regression", "Regression Tree", "Random Forest", 
            "Support Vector Machine","Neural Network")
```

### MSE value of corresponding models above
```{r}
Test.MSE<- c(MSE.Test.lm, MSE.Test.tree, MSE.Test.rf, MSE.Test.svm, MSE.Test.nm)
```

### Create a FataFrame with model type and MSE (round the MSE values to 4 decimal places).
```{r}
Test.result<-data.frame(Model.Type, Test.MSE)
print(Test.result, digits = 4)
```

## **As the Random Forest model has the lowest MSE (calculated using the test data), this model is the best model among the models above.**

## **But, I would recommend Regression tree model (Step 3-d) as the best model to use to explain the label values based on features as it is straight forward and simple to grasp. Disadvantage of this model over other models is that it is not the best model in terms of accuracy of prediction as shown above with the calculated test error (MSE).** 

## Predict the insurance charges of new individuals.

```{r}
age <- as.numeric(c(60, 20))
sex<- as.character(c("female", "male"))
bmi<- as.numeric(c(35.8, 28.02))
children <- as.numeric(c(0, 1))
smoker<- as.character(c("no", "yes"))
region <- as.character(c("northeast","northwest"))
predict_df <- data.frame(age, sex, bmi, children, smoker, region)
head(predict_df)
```

###  Convert the predictor variables "sex", "smoker" and "region" to factor.
```{r}
predict_df$sex<-factor(predict_df$sex)
predict_df$smoker<-factor(predict_df$smoker)
predict_df$region<-factor(predict_df$region)
```

### Match the factor labels of predict_df factors with the respective variable in training data. Random forest may show error if the factor level does not match.
```{r}
levels(predict_df$sex) <- rf.model$forest$xlevels$sex
levels(predict_df$smoker) <- rf.model$forest$xlevels$smoker
levels(predict_df$region) <- rf.model$forest$xlevels$region
```

## Predict the insurance charges based on feature variables. I will need to convert the log tranformed charges values back to actual charges by using exp() to get the actual predicted insurance charge.

```{r}
predict_insurance_charge <- exp(predict(rf.model, newdata = predict_df))
predict_insurance_charge
```





























